{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = [' ' for _ in range(9)]  # Use a single list to represent 3x3 board\n",
    "        self.current_winner = None  # Keep track of winner\n",
    "\n",
    "    def print_board(self):\n",
    "        for row in [self.board[i*3:(i+1)*3] for i in range(3)]:\n",
    "            print('| ' + ' | '.join(row) + ' |')\n",
    "\n",
    "    def available_moves(self):\n",
    "        return [i for i, spot in enumerate(self.board) if spot == ' ']\n",
    "\n",
    "    def empty_squares(self):\n",
    "        return ' ' in self.board\n",
    "\n",
    "    def num_empty_squares(self):\n",
    "        return self.board.count(' ')\n",
    "\n",
    "    def make_move(self, square, letter):\n",
    "        if self.board[square] == ' ':\n",
    "            self.board[square] = letter\n",
    "            if self.winner(square, letter):\n",
    "                self.current_winner = letter\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def winner(self, square, letter):\n",
    "        # Check row, column, and diagonal\n",
    "        row_ind = square // 3\n",
    "        row = self.board[row_ind*3 : (row_ind + 1) * 3]\n",
    "        if all([s == letter for s in row]):\n",
    "            return True\n",
    "\n",
    "        col_ind = square % 3\n",
    "        column = [self.board[col_ind+i*3] for i in range(3)]\n",
    "        if all([s == letter for s in column]):\n",
    "            return True\n",
    "\n",
    "        if square % 2 == 0:  # Check diagonals\n",
    "            diagonal1 = [self.board[i] for i in [0, 4, 8]]\n",
    "            if all([s == letter for s in diagonal1]):\n",
    "                return True\n",
    "            diagonal2 = [self.board[i] for i in [2, 4, 6]]\n",
    "            if all([s == letter for s in diagonal2]):\n",
    "                return True\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class QAgent:\n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):\n",
    "        self.q_table = {}  # Q(s,a) table\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def get_state_key(self, board):\n",
    "        return ''.join(board)\n",
    "\n",
    "    def choose_action(self, state, available_moves):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(available_moves)  # Explore\n",
    "        else:\n",
    "            q_values = [self.get_q_value(state, move) for move in available_moves]\n",
    "            max_q = max(q_values)\n",
    "            if q_values.count(max_q) > 1:\n",
    "                best_moves = [i for i in range(len(available_moves)) if q_values[i] == max_q]\n",
    "                i = random.choice(best_moves)\n",
    "            else:\n",
    "                i = q_values.index(max_q)\n",
    "            return available_moves[i]\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table.get((state, action), 0.0)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        current_q = self.get_q_value(state, action)\n",
    "        max_next_q = max([self.get_q_value(next_state, a) for a in TicTacToe().available_moves()]) if TicTacToe().available_moves() else 0\n",
    "        new_q = current_q + self.lr * (reward + self.gamma * max_next_q - current_q)\n",
    "        self.q_table[(state, action)] = new_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Episode: 1000\n",
      "Episode: 2000\n",
      "Episode: 3000\n",
      "Episode: 4000\n",
      "Episode: 5000\n",
      "Episode: 6000\n",
      "Episode: 7000\n",
      "Episode: 8000\n",
      "Episode: 9000\n",
      "| O |   |   |\n",
      "|   |   |   |\n",
      "|   |   |   |\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'choose_action'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m trained_agent \u001b[38;5;241m=\u001b[39m train()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Test against human\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[43mplay_with_human\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_agent\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 47\u001b[0m, in \u001b[0;36mplay_with_human\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     45\u001b[0m env\u001b[38;5;241m.\u001b[39mmake_move(move, human_player)\n\u001b[1;32m     46\u001b[0m env\u001b[38;5;241m.\u001b[39mprint_board()\n\u001b[0;32m---> 47\u001b[0m winner \u001b[38;5;241m=\u001b[39m \u001b[43mplay_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m winner \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTie\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwinner\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m won!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m, in \u001b[0;36mplay_game\u001b[0;34m(p1, p2, env, draw)\u001b[0m\n\u001b[1;32m      4\u001b[0m current_player \u001b[38;5;241m=\u001b[39m p1 \u001b[38;5;28;01mif\u001b[39;00m current_player \u001b[38;5;241m!=\u001b[39m p1 \u001b[38;5;28;01melse\u001b[39;00m p2\n\u001b[1;32m      5\u001b[0m state_key \u001b[38;5;241m=\u001b[39m p1\u001b[38;5;241m.\u001b[39mget_state_key(env\u001b[38;5;241m.\u001b[39mboard)\n\u001b[0;32m----> 6\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mcurrent_player\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m(state_key, env\u001b[38;5;241m.\u001b[39mavailable_moves())\n\u001b[1;32m      7\u001b[0m env\u001b[38;5;241m.\u001b[39mmake_move(action, current_player_letter(current_player))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m draw:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'choose_action'"
     ]
    }
   ],
   "source": [
    "def play_game(p1, p2, env, draw=False):\n",
    "    current_player = None\n",
    "    while env.empty_squares():\n",
    "        current_player = p1 if current_player != p1 else p2\n",
    "        state_key = p1.get_state_key(env.board)\n",
    "        action = current_player.choose_action(state_key, env.available_moves())\n",
    "        env.make_move(action, current_player_letter(current_player))\n",
    "        if draw:\n",
    "            env.print_board()\n",
    "        if env.current_winner:\n",
    "            if draw:\n",
    "                print(f\"Player {env.current_winner} wins!\")\n",
    "            return env.current_winner\n",
    "    if draw:\n",
    "        print(\"It's a tie!\")\n",
    "    return 'Tie'\n",
    "\n",
    "def current_player_letter(player):\n",
    "    return 'X' if isinstance(player, QAgent) else 'O'\n",
    "\n",
    "# Training the agent\n",
    "def train(episodes=10000):\n",
    "    player1 = QAgent()\n",
    "    player2 = QAgent()\n",
    "    env = TicTacToe()\n",
    "    for episode in range(episodes):\n",
    "        if episode % 1000 == 0:\n",
    "            print(f\"Episode: {episode}\")\n",
    "        play_game(player1, player2, env)\n",
    "        env = TicTacToe()  # Reset environment after each game\n",
    "    return player1\n",
    "\n",
    "# Play against human\n",
    "def play_with_human(agent):\n",
    "    env = TicTacToe()\n",
    "    human_player = 'O'\n",
    "    while env.empty_squares():\n",
    "        if human_player == 'O':\n",
    "            move = int(input(\"Enter your move (0-8): \"))\n",
    "            if move not in env.available_moves():\n",
    "                print(\"Invalid move. Try again.\")\n",
    "                continue\n",
    "        else:\n",
    "            move = agent.choose_action(agent.get_state_key(env.board), env.available_moves())\n",
    "        env.make_move(move, human_player)\n",
    "        env.print_board()\n",
    "        winner = play_game(agent, 'human', env)\n",
    "        if winner != 'Tie':\n",
    "            print(f\"{winner} won!\")\n",
    "            break\n",
    "        human_player = 'X' if human_player == 'O' else 'O'\n",
    "\n",
    "# Train the agent\n",
    "trained_agent = train()\n",
    "\n",
    "# Test against human\n",
    "play_with_human(trained_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
